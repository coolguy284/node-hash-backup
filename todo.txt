verify command

update readme.md to match help output
fix all linter errors
ensure test still runs

improved data storage on windows:
  support for directory junction / file and directory symlinks
    remove warning in help about symbolic link type

todo for any code validation pass after commit:
update readme.md to match help output
run "npx eslint ."
run "npm test symlink"

low prio:
call hb dispose if program termination
backup cmd has name option but also namePrefix; if nameprefix is set, createbackup will return name of new backup; and will mass rename backups to leading zeros version upon power of 10; createbackup also have inheritFrom option; and a set unset backup inheritance separate subcommand; that can also link in sequence given prefix as well
use flock to ensure that only current process/(specific backup manager instance?) has control over edit.lock, and allow edit.lock to already exist as long as it is empty
create native module to do what powershell cannot (replace setFileTimes and getSymlinkType with versions that call native module if it exists)
improved data storage on windows:
  save hidden, system flags
  detect alternate file streams on windows; store as path with ':'
list command:
  add percentage compression to list command as the ratio between compressed size and size; add pseudo percentage compression as the ratio between total (deduplicated total; not grand total) and artifical sum; also percentage as the ratio between regular total and grand total, so percentage of all file bytes that is not meta
eslint require jsdoc, and also just do the jsdoc for all funcs
rework getentryinfo to show whether compression is enabled as well, and also the compression algo and params used
function param and subcommand param in backup to create backup with new name following prefix instead of full name
implement proper async operation read/write locking in backup manager class
add ctrl c lock to certain parts of code (look up good way), to ensure any backup or load or other operation isnt interrupted during write
internal backup dir command to store file by repeatable stream func or by bytes, then:
  createbackupview and importbackup
  then function to transfer one/many/all backups from one backupmanager to another; efficiently copies from one backup to another
    with in memory cutoff like usual
  then transmute

if list command is slow: optimize command to run much faster, potentially cache info in an indexed manner, putting into backupinfo.json inside cache folder in backup
improved data storage:
  explicitly detect hardlinks
create helper funcs and cli commands for all aspects of backupmanager; including the low-level hex stuff
add reverse search by hash for what backups and filenames in the backup have the file
more options to define backup granularity:
  store file times?
  store file attributes?
  etc.
option to reset access times after finishing backing up a folder
sort metafiles as adding new entries to them
version 3
  handle duplicate hashes
    //'    --check-duplicate-hashes (default false): If true, check for whether files are truly equal if their hashes are (false not implemented yet, true will error if hashes match as duplicate hash handling not implemented yet).\n'
  use "./path" for all paths not root to remove edge case
  hash backup version 3 can have 2 modes, human readable json files and minified compressed json files
  the readonly optional files in v2 are required readonly in v3
  store compress level and other params in file metadata json, to know if need to compress file more if upgrading archive compress level
  option (maybe true by default?) to write protect all files in archive, and unprotect them when being edited only
  add creationtime to file_meta json files
  folders in backup folder for categories
  support inheritance from another backup entry, specifying only created, updated (incl. modtimes, attributes), or deleted files/folders
    support linking a backup to another or unlinking it, after backup is created
  store immutable, appendonly attributes on linux
  low prio:
    add checksums
modify command that can modify backup dir parameters
  first step modifies hash slice length, hash slices count, and hash type (if set)
    first 2 can be done at once, only problem renaming if from hash slice length of hash and hash slices count of 1, to hash slices count of 0, or other way around; if so, temporarily add character to hash file names then mkdir or rmdir and rename
  second step modifies compression algorythm and parameters (if set), useful to recompress dir after the fact to a much higher compression level, can be set to change base dir compress level or just change compression of files in dir with new files coming in still having old compression level or both

verify function to verify hash backup dir with no extraneous parts
  verifies contents of hash dir, erroring if there is extra stuff there or things are invalid
  verify command for whole hash backup or optionally just one backup
  alternate mode that is functionally equivalent to restoring and checksumming but does so without restoring
    add variant that will calculate hash and hash with names of slice of it to exactly match restoring it and calculating hash
copy over imports of upgrader.mjs into the file itself to isolate the imports and protect against accidental update of the functions
hashslices in info.json maybe should be array like [[0,2],[2,4]] with each entry referring to a folder name slice from the hash of the file
should be able to have at least 1 archive file (like zip, tar, wim, gz (yes single file ones count, particularly for tgz)) in folder paths like basepath for backup and restore, backupdir, excludedir, restoresubpath; path for view backup files subpath and other commands
  do 7zip style for paths of targz, ex. file.tgz/file.tar/insidefolder
file compression reverse engineering
  attempt to reverse engineer compression of already compressed files and then decompress and recompress with better algorythm
  do for gzip bzip2 lzma png jpg
  use jxl for images lzma for files, recompress low power lzma with high power
  recompress only if compress params of orig file can be determined to get exact file back
  for jpg, jxl should be able to always reversibly recompress
    bmp would probably also be a guaranteed compress and decompress byte perfect
  backup param acceptaltereddecompresspaths where those paths if files or subpaths of those paths if folders will accept a byte imperfect decompress as long as original data (like pixels for the image or files inside and metadata for the archive) is still preserved
    whitelist and blacklist of file extensions or filetypes maybe instead to even bother compressing (compresstypes), or for acceptaltereddecompress but for types instead
  if reconstruction is really close store bytes diff, or even if not really close but storing byte diff would still be a net savings
  attempt to compress files inside zips and other archives directly if archive can be perfectly reconstructed or near perfectly with byte diffs still being a net savings
  file meta for zips should have an array of all file hashes inside zip to ensure that those file hashes in the archive are not accidentally counted as orphaned files (this is only if zip was unpacked in first place)
  whether zip was unpacked should be flag on the file meta of the zip so it is known without having to exhaustively inventory every file in backup
  file meta compression should be array of "steps" to make orig file, ex. first take the file in the archive and brotlidecompress then compress with gz then apply byte diffs
  backup command have param for limit of nested zips to try and unpack, from 0 to inf default idk; also argument for limit of zips in filepath of things
  backup command have bytes limit on decompressing files output length (in attempt to decompress and reverse engineer), to prevent using up harddisk or memory decompressing a 10kb input 1tb output gzip file and then recompressing it; the param should go from -1 to not even try reverse engineering anything (this will supersede nested zips setting and always prevent unpacking) to infinity
  for targz try unpacking tar and if not try only reverse engineering gz
  same for tarbz2 tarxz etc
compress stored json files
add comments
add docs for node api
create test that tests each facet of each feature
improve readme
add max-compress-cutoff above which no max compression
option to perform verification of entire backup dir before performing backup
add option to change hash digest encoding (currently just hex)
add ultimate compression mode that tests every algo with max settings and uses the one with the smallest filesize
make sure program and modtime setting and getting works on linux
allow subfolders in backups folder
pendingoperations in info.json or pending_ops.json that stores current operation like modify so it can continue if stopped, new complete_operation command solely to resume this pending operation, other backup commands dont work until operation finished
restore function will have 3 modes, normal mode, and hardlink and symlink modes that create hardlinks or symlinks instead, and they link to a cache folder in the archive dir that has files_uncompressed in it that has uncompressed versions of each file (or hardlinks to the file in the normal place if it is already uncompressed)
  option to always create copy of file in cached folder, therefore deleting the cache folder (when mode set to hardlink) will "seperate" the restored files from any new restores done (otherwise 2 files with same contents in the different restores will be linked)
  warning with hardlink and symlink modes that any alteration of the files might alter files in the backup (and thus corrupt it), if option to always create copy not set; if option to always create copy set or copy made anyway due to compression, still possibility of modifying one file causing files in the cache, in other restores, and in the same restore with the same content to be modified as well
more plumbing-related commands and helper funcs, like:
  getFileStreamByHex
  getFilesWithHexPrefix
  getCountOfFilesWithHexPrefix
fix inconsistent periods at the end of console logged messages
use custom error type when throwing, and print custom errors differently (without the traceback)
function to delete certain files in a backup instead of whole backup
fix main and exports in package json, shouldnt need main
add prompt to confirm to delete backup dir contents if backup dir not empty on calling init
add prompt to confirm deletion of original restore dir contents, instead of just setting it in cli arg
clean up exports in pkg json a bit
investigate why using this in fullBackupInfoDump causes 9GB of data to be placed in arraybuffer:
  const backupInfo = await Promise.all(
    backupNames.map(
      async backupName => [
        backupName,
        await this.singleBackupInfoDump(backupName, { summary: false }),
      ]
    )
  );
add crc32 and crc64 as supported hashes

unnecessary / difficult:
make sure all async subfunctions / function calls in every async function are awaited
check to make sure stopping program halfway does not cause broken data state anywhere

not doing:
wipe backup dir command should only delete files and dirs with backup content in them
