cache filemeta as well
option to disable backupmgr cache
make tests more fully deterministic by creating random folders sequentially in order

add above-compress-cutoff above which no compression at all, and below cutoff for no compression below a certain size (although rarely needed)
option to reset access times after finishing backing up a folder
add reverse search by hash for what backups and filenames in the backup have the file
internal backup dir command to store file by repeatable stream func or by bytes
function to transfer one/many/all backups from one backupmanager to another; efficiently copies from one backup to another
  with in memory cutoff like usual, only if the cutoff is even needed
add option to backup for checkForDuplicateHashes, default true
add option to restore for verifyExportedFileHashes, default true

delete backup and delete backup dir must have confirm=yes
fix inconsistent periods at the end of console logged messages
use custom error type when throwing, and print custom errors differently (without the traceback)

update readme.md and help output to match true commands
  particularly add 'remove' command '--setFileTimes' option

create command line access for each command in backup manager class within reason
clear TODO from codebase
implement proper locking in backup manager class
implement tests
extra command line arg for interactive mode that opens a node shell with hb as the var name of the BackupManager
support sliced hash algos and xof hash funcs with optional extra slicing
  add hashParams obj to info that has slice param and original length param (for variable length hashes only)
support inheritance from another backup entry, specifying only created, updated (incl. modtimes, attributes), or deleted files/folders
  support linking a backup to another or unlinking it, after backup is created
add verify function which verifies contents of hash dir, erroring if there is extra stuff there or things are invalid
support for directory junction / file and directory symlinks
subcommands and command line options for all facets of the backup dir functions including defaults
  support for specifying parameters for the compression algos
  specify command arguments in --arg=value format only
  if necessary, use command line params with no value where presence is enough to specify action (add in support for this anyway)
  view command to view file hierarchy of backup
  list command for listing backups (also lists basic stats like number of files, folders, entries, and total size)
    add percentage compression to list command as the ratio between compressed size and size; add pseudo percentage compression as the ratio between total (deduplicated total; not grand total) and artifical sum; also percentage as the ratio between regular total and grand total, so percentage of all file bytes that is not meta
  backup command should autocomplete new backup name with optional discriminatior (like first_backup_type); only enable this behavior if flag set to true
configurable lockfile behavior, either error out or just wait until lockfile is deleted using watchfile
cache meta datas accessed
optimize backup getinfo command to run much faster, potentially cache info in an indexed manner, putting into backupinfo.json inside cache folder in backup
ensure able to back up single file into hash backup

createbackupview and importbackup
then transfer
add ctrl c lock to certain parts of code (look up good way), to ensure any backup or load or other operation isnt interrupted during write
on helper funcs, properly try finally the resource after successful creation of backupmanager
fix symlinkmode on restore

improved data storage:
  support for file attrs like readonly, appendonly, immutable, etc.
  add alternate stream support on windows, straightforwardly by storing paths with ':'
  explicitly detect hardlinks

low prio:
version 3
  handle duplicate hashes
    //'    --check-duplicate-hashes (default false): If true, check for whether files are truly equal if their hashes are (false not implemented yet, true will error if hashes match as duplicate hash handling not implemented yet).\n'
  hash backup version 3 can have 2 modes, human readable json files and minified compressed json files
  the readonly optional files in v2 are required readonly in v3
  store compress level and other params in file metadata json, to know if need to compress file more if upgrading archive compress level
  option (maybe true by default?) to write protect all files in archive, and unprotect them when being edited only
  add creationtime to file_meta json files
  backup categories
  low prio:
    add checksums
add lzma support to node-hash-backup as an optional dependency
modify command that can modify backup dir parameters including upgrading or downgrading hash backup format
  first step modifies hash slice length, hash slices count, and hash type (if set)
    first 2 can be done at once, only problem renaming if from hash slice length of hash and hash slices count of 1, to hash slices count of 0, or other way around; if so, temporarily add character to hash file names then mkdir or rmdir and rename
  second step modifies compression algorythm and parameters (if set), useful to recompress dir after the fact to a much higher compression level, can be set to change base dir compress level or just change compression of files in dir with new files coming in still having old compression level or both

verify function to verify ver 1 hash backup dir with no extraneous parts; also for ver 2
  verify command for whole hash backup or optionally just one backup
  alternate mode that is functionally equivalent to restoring and checksumming but does so without restoring
    add variant that will calculate hash and hash with names of slice of it to exactly match restoring it and calculating hash
copy over imports of upgrader.mjs into the file itself to isolate the imports and protect against accidental update of the functions
hashslices in info.json maybe should be array like [[0,2],[2,4]] with each entry referring to a folder name slice from the hash of the file
should be able to have at least 1 archive file (like zip, tar, wim, gz (yes single file ones count, particularly for tgz)) in folder paths like basepath for backup and restore, backupdir, excludedir, restoresubpath; path for view backup files subpath and other commands
  do 7zip style for paths of targz, ex. file.tgz/file.tar/insidefolder
file compression reverse engineering
  attempt to reverse engineer compression of already compressed files and then decompress and recompress with better algorythm
  do for gzip bzip2 lzma png jpg
  use jxl for images lzma for files, recompress low power lzma with high power
  recompress only if compress params of orig file can be determined to get exact file back
  for jpg, jxl should be able to always reversibly recompress
    bmp would probably also be a guaranteed compress and decompress byte perfect
  backup param acceptaltereddecompresspaths where those paths if files or subpaths of those paths if folders will accept a byte imperfect decompress as long as original data (like pixels for the image or files inside and metadata for the archive) is still preserved
    whitelist and blacklist of file extensions or filetypes maybe instead to even bother compressing (compresstypes), or for acceptaltereddecompress but for types instead
  if reconstruction is really close store bytes diff, or even if not really close but storing byte diff would still be a net savings
  attempt to compress files inside zips and other archives directly if archive can be perfectly reconstructed or near perfectly with byte diffs still being a net savings
  file meta for zips should have an array of all file hashes inside zip to ensure that those file hashes in the archive are not accidentally counted as orphaned files (this is only if zip was unpacked in first place)
  whether zip was unpacked should be flag on the file meta of the zip so it is known without having to exhaustively inventory every file in backup
  file meta compression should be array of "steps" to make orig file, ex. first take the file in the archive and brotlidecompress then compress with gz then apply byte diffs
  backup command have param for limit of nested zips to try and unpack, from 0 to inf default idk; also argument for limit of zips in filepath of things
  backup command have bytes limit on decompressing files output length (in attempt to decompress and reverse engineer), to prevent using up harddisk or memory decompressing a 10kb input 1tb output gzip file and then recompressing it; the param should go from -1 to not even try reverse engineering anything (this will supersede nested zips setting and always prevent unpacking) to infinity
  for targz try unpacking tar and if not try only reverse engineering gz
  same for tarbz2 tarxz etc
compress stored json files
add comments
add docs for node api
create test that tests each facet of each feature
improve readme
add max-compress-cutoff above which no max compression
option to perform verification of entire backup dir before performing backup
add option to change hash digest encoding (currently just hex)
add ultimate compression mode that tests every algo with max settings and uses the one with the smallest filesize
make sure program and modtime setting and getting works on linux
allow subfolders in backups folder
pendingoperations in info.json or pending_ops.json that stores current operation like modify so it can continue if stopped, new complete_operation command solely to resume this pending operation, other backup commands dont work until operation finished
restore function will have 3 modes, normal mode, and hardlink and symlink modes that create hardlinks or symlinks instead, and they link to a cache folder in the archive dir that has files_uncompressed in it that has uncompressed versions of each file (or hardlinks to the file in the normal place if it is already uncompressed)
  option to always create copy of file in cached folder, therefore deleting the cache folder (when mode set to hardlink) will "seperate" the restored files from any new restores done (otherwise 2 files with same contents in the different restores will be linked)
  warning with hardlink and symlink modes that any alteration of the files might alter files in the backup (and thus corrupt it), if option to always create copy not set; if option to always create copy set or copy made anyway due to compression, still possibility of modifying one file causing files in the cache, in other restores, and in the same restore with the same content to be modified as well

unnecessary / difficult:
make sure all async subfunctions / function calls in every async function are awaited
check to make sure stopping program halfway does not cause broken data state anywhere

not doing:
wipe backup dir command should only delete files and dirs with backup content in them
